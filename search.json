[{"title":"InfluxDB","url":"/2018/06/12/InfluxDB/","content":"\n# 1 概览\nInfluxDB是一个开源分布式时序、事件和指标数据库。使用go语言编写，无需外部依赖。其设计目标是实现分布式和水平伸缩扩展。  \n它有三大特性：  \n\n1. Time Series （时间序列）：你可以使用与时间有关的相关函数（如最大，最小，求和等  \n2. Metrics（度量）：你可以实时对大量数据进行计算  \n3. Eevents（事件）：它支持任意的事件数据  \n\n# 2 特点  \n\n- schemaless(无结构)，可以是任意数量的列  \n- scalable（可扩展 ）\n- min, max, sum, count, mean, median 一系列函数，方便统计  \n- Native HTTP API, 内置http支持，使用http读写  \n- Powerful Query Language 类似sql  \n- Built-in Explorer 自带管理工具  \n\n# 3 API\n\nInfluxDB 支持两种api方式：\n\n- Http API\n- Protobuf API\n\n# 4 查询语言  \n\n\nInfluxDB 提供了类似sql的查询语言：\n\n``` \n    select * from events where state == 'NY';\n    \n    select * from log_lines where line =~ /error/i;\n    \n    select * from events where customer_id == 23 and type == 'click';\n    \n    select * from response_times where value > 500;\n    \n    select * from events where email !~ /.*gmail.*/;\n    \n    select * from nagios_checks where status != 0;\n    \n    select * from events \n    where (email =~ /.*gmail.* or email =~ /.*yahoo.*/) and state == 'ny';\n    \n    delete from response_times where time > now() - 1h    \n```\n\n\n非常容易上手, 还支持Group By, Merging Series, Joining Series， 并内置常用统计函数，比如max, min, mean 等\n\n# 5 库  \n\n常用语言的库都有，因为api简单，也很容易自己封装。\n\nInfluxdDB作为很多监控软件的后端，这样监控数据就可以直接存储在InfluxDB。比如：StatsD, CollectD, FluentD。\n\n还有其它的可视化工具支持InfluxDB, 这样就可以基于InfluxDB很方便的搭建监控平台\n\n# 6 InfluxDB 数据可视化工具  \n\nInfluxDB 用于存储基于时间的数据，比如监控数据，因为InfluxDB本身提供了Http API，所以可以使用InfluxDB很方便的搭建了个监控数据存储中心。\n\n对于InfluxDB中的数据展示，官方admin有非常简单的图表, 看起来是这样的:\n\n\n![](/images/influxdb-2.jpg)\n\n除了自己写程序展示数据还可以选择：\n\n- tasseo https://github.com/obfuscurity/tasseo/  \n- grafana https://github.com/torkelo/grafana  \n\n## 6.1 tasseo  \n\ntasseo,为Graphite写的Live dashboard，现在也支持InfluxDB,tasseo 比较简单, 可以配置的选项很少。\n\n\n![](/images/influxdb-3.png)\n\n## 6.2 Grafana  \n\nGrafana是一个纯粹的html/js应用，访问InfluxDB时不会有跨域访问的限制。只要配置好数据源为InfluxDB之后就可以，剩下的工作就是配置图表。Grafana 功能非常强大。使用ElasticsSearch保存DashBoard的定义文件，也可以Export出JSON文件(Save ->Advanced->Export Schema)，然后上传回它的/app/dashboards目录。 \n\n配置数据源:\n\n```\n     datasources: {      \n          influx: {\n            default: true,\n            type: 'influxdb',\n            url: 'http://<your_influx_db_server>:8086/db/<db_name>',\n            username: 'test',\n            password: 'test',\n          }\n        },        \n```\n\n\n最后可以看到如下图所示界面：\n\n![](/images/influxdb-4.png)\n\n\n# 7 参考\nhttp://www.ttlsa.com/monitor-safe/monitor/distributed-time-series-database-influxdb/\n\n\n\n\n","tags":["go"]},{"title":"MongoDB与HBase比较","url":"/2018/06/12/MongoDB与HBase比较/","content":"\n# 一、概览\nMongoDB与HBase都是技术领先的非关系性数据库。MongoDB使用C++编写，HBase使用Java编写，HBase会受Java GC暂停的影响。MongoDB将数据存储为BSON（JSON的二进制形式表示）的文档。HBase专为 具有随机读取和写入访问模式Key-Value工作负载设计。\n\n# 二、关键概念\n许多关系数据库概念与MongoDB和HBase有相似之处。该表概述了每个系统中的一些常见概念。  \n\n|RDBMS|MongoDB|Hbase|\n|---|---|---|  \n| 表| collection|表 |\n| 行| Document| 列族|\n| 没有类似的| 分片|分区 |\n| GROUP BY| Aggregation Pipeline| MapReduce|\n| 多记录的ACID事务|多记录的ACID事务 | 没有|\n\n# 特性\n## 开发者关注特性  \n\n| MongoDB|HBase|\n|---|---|\n| __数据模型__  |Document |宽列 |\n| __支持的数据类型__ |多种|数据转化为二进制|\n|  __查询模型__ | 多功能的查询|Key-Value |\n|  __二级索引__ |支持 |需要开发人员自己解决 |\n|  __聚合__ |支持 |需要将数据移动到特定的数据分析框架中 |\n|  __文本搜索__ | 支持|需要将数据转移到的特定的数据分析框架中 |\n|  __数据保证__ | 支持| 没有|\n|  __构建响应、事件驱动应用__ | 支持| 没有|\n| __驱动支持__  |11种支持的驱动和30+社区支持 |支持Java和Thrift |\n| __事务保证__  | 支持简单的事务| 单一行原子操作|\n\n## 管理者关注特性  \n\n|  |MongoDB|HBase|  \n|--|--|--|  \n| __创建产品级集群需要的最少节点__ |3个：主节点和次节点 |10个：主和次级HMaster,RegionServers、hdfs和zookeeper |  \n|  __推荐每个节点存储的最大数据量__ |没有限制 |4TB |  \n| __主节点失效恢复时间__ |2秒，数据可在次级节点读取 |60秒，数据可在次级节点读取 |  \n|  __性能维护__ |C++编写不会出现Java GC暂停 |Java 编写，GC会出现暂停的问题|  \n| __数据分区__ | 支持hash、range、zone|仅支持hash |  \n| __备份和恢复__|||  \n|  __Spark和Hadoop适应__ |支持 |支持 |\n\n# 参考\n1. https://www.mongodb.com/compare/mongodb-hbase\n","tags":["HBase"]},{"title":"常见的几种开源协议","url":"/2018/06/12/常见的几种开源协议/","content":"\n# 引言\n说到开源协议，不得不提GNU。GNU is Not Unix”，这是官方给出的递归定义，永远也找不到本意，我们可以将它理解为一个自由软件工程项目或者一种计划，是由Richard Stallman在1983年9月27公开发起的，它的目标是创建一套完全自由、开放的操作系统。1985年10月Richard Stallman创立了自由软件基金会（Free Software Foundation ，FSF），其主要工作是执行GNU计划。为了保证GNU软件可以自由的“使用、复制、修改、发布”，同样也禁止部分人在GNU软件的基础上自己修改并发布的软件中添加任何限制他人自由使用的条款，在这个情况下就诞生了GNU的许可条款，再后来又产生了其他的（非GNU）许可条款，统称为开源许可协议。\n\n常见的几种开源协议如下：\n\n- GPL （GNU General Public License） ：GNU通用公共许可协议\n- LGPL （GNU Lesser General Public License） ：GNU宽通用公共许可协议\n- BSD (Berkeley Software Distribution)  :伯克利软件分发许可协议\n- MIT （Massachusetts Institute of Technology）：MIT许可协议之名源自麻省理工学院，又称“X许可协议”或“X11许可协议”\n- Apache （Apache License） ：Apache许可协议\n- MPL （Mozilla Public License） ：Mozilla公共许可协议\n\n# GPL\nGNU通用公共许可协议是一个被广泛使用的自由软件许可协议条款，GPL 保证了所有开发者的权利，同时为使用者提供了足够的复制，分发，修改的权利。\n\n- 可自由复制\n- 可自由分发\n- 可以用来盈利\n- 可自由修改\n\n# LGPL\nGNU 还有另外一种协议，叫做GNU宽通用公共协议，它对产品所保留的权利比 GPL 少，总的来说，LGPL 适合那些用于非 GPL 或非开源产品的开源类库或框架。因为GPL要求包含有部分GPL授权代码的软件以GPL方式发布，这样开发者就无法在收费的专属软件里使用GPL授权代码。 LGPL正好解决了这一问题：它不要求其它使用LGPL授权代码的软件以LGPL方式发布。注意：LGPL有一特点是LGPL软件可以被转换成GPL。这种特性对于在GPL库或应用程序中直接使用LGPL程序有一定程度之帮助。\n\n# BSD\nBSD 在软件分发方面的限制比别的开源协议（如 GNU GPL）要少。该协议有多种版本，最主要的版本有两个，新 BSD 协议与简单 BSD 协议，这两种协议经过修正，都和 GPL 兼容，并为开源组织所认可。\n\n新 BSD 协议在软件分发方面，除需要包含一份版权提示和免责声明之外，没有任何限制。另外，该协议还禁止拿开发者的名义为衍生产品背书，但简单 BSD 协议删除了这一条款。\n\n# MIT\nMIT 协议可能是几大开源协议中最宽松的一个，核心条款是：\n\n该软件及其相关文档对所有人免费，可以任意处置，包括使用，复制，修改，合并，发表，分发，再授权，或者销售。唯一的限制是，软件中必须包含上述版 权和许可提示。这意味着：\n\n- 你可以自由使用，复制，修改，可以用于自己的项目。\n- 可以免费分发或用来盈利。\n- 唯一的限制是必须包含许可声明。\n\nMIT 协议是所有开源许可中最宽松的一个，除了必须包含许可声明外，再无任何限制。\n\n# Apache\n\nApache 协议 2.0 和别的开源协议相比，除了为用户提供版权许可之外，还有专利许可，对于那些涉及专利内容的开发者而言，该协议最适合。\n\nApache 协议还有以下需要说明的地方:\n\n- 永久权利\n- 全球范围的权利\n- 授权免费，且无版税\n- 授权无排他性\n- 授权不可撤消\n\n分发代码方面包含一些要求，主要是，要在声明中对参与开发的人给予认可并包含一份许可协议原文。\n\n# MPL\nMPL既是得到自由软件基金会承认的自由软件许可证，也是得到开放源代码促进会承认的开源软件许可证。MPL允许在其授权下的源代码与其他授权的文件进行混合，包括私有许可证。但在MPL授权下的代码文件必须保持MPL授权，并且保持开源。这样的条款让MPL既不像MIT和BSD那样允许派生作品完全转化为私有，也不像GPL那样要求所有的派生作品，包括新的组件在内，全部必须保持GPL。通过允许在派生项目中存在私有模块，同时保证核心文件的开源，MPL同时激励了商业及开源社区来参与帮助开发核心软件。\n\n使用MPL授权的软件并不受专利的限制，其可以自由使用，修改，并可自由的重新发布。带有专利代码的版本仍然可以使用，转让，甚至出售，但未经许可则不能修改代码。此外，MPL并不授予用户对于开发者商标的使用权。\n\n为了满足MPL的条款限制，用户必须负担一些“责任”，主要是关于散发使用MPL授权的软件。用户必须确保重新散发的软件所有源代码均以MPL授权，即使是以可执行文件的方式提供或是与其他使用专有软件授权的源代码结合也一样。但若跟以GNU通用公共许可协议、GNU宽通用公共许可证、Affero通用公共许可证授权的源代码结合则是例外。此时开发者则可选用以上三种更加严格的条款来授权。\n\n# 参考\nhttps://blog.csdn.net/u014680729/article/details/24382261\n\n","tags":["对比"]},{"title":"avro","url":"/2018/06/12/avro/","content":"\n# 1 介绍\nAvro（[ævrə]）是Hadoop的一个子项目，由Hadoop的创始人Doug Cutting（也是Lucene，Nutch等项目的创始人）牵头开发。Avro是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro提供的机制使动态语言可以方便地处理Avro数据.\nAvro支持两种序列化编码方式：二进制编码和JSON编码.使用二进制编码会高效序列化，并且序列化后得到的结果会比较小；而JSON一般用于调试系统或是基于WEB的应用。对Avro数据序列化/反序列化时都需要对模式以深度优先(Depth-First)，从左到右(Left-to-Right)的遍历顺序来执行。Avro依赖模式(Schema)来实现数据结构定义。可以把模式理解为Java的类，它定义每个实例的结构，可以包含哪些属性。可以根据类来产生任意多个实例对象。对实例序列化操作时必须需要知道它的基本结构，也就需要参考类的信息。这里，根据模式产生的Avro对象类似于类的实例对象。每次序列化/反序列化时都需要知道模式的具体结构。所以，在Avro可用的一些场景下，如文件存储或是网络通信，都需要模式与数据同时存在。Avro数据以模式来读和写(文件或是网络)，并且写入的数据都不需要加入其它标识，这样序列化时速度快且结果内容少。由于程序可以直接根据模式来处理数据，所以Avro更适合于脚本语言的发挥。\n\n# 2 使用\n\n## 2.1 需要的Jar包依赖\n\navro-1.7.3.jar，avro-tools-1.7.3.jar，jackson-core-asl-1.9.3.jar，jackson-mapper-asl-1.9.3.jar  \n\n## 2.2 定义模式\n\n在avro中，它是用Json格式来定义模式的。模式可以由基础类型（null, boolean, int, long, float, double, bytes, and string）和复合类型(record, enum, array, map, union, and fixed)的数据组成。这里定义了一个简单的模式user.avsc:\n\n```\n{\n\t\"namespace\": \"com.zq.avro\",   \n\t\"type\": \"record\",    \n\t\"name\": \"User\",  \n\t \"fields\": [ \n\t\t\t{\n\t\t\t\t\"name\": \"name\",   \n\t\t\t\t\"type\": \"string\"\n\t\t\t}, \n\t\t\t{\n\t\t\t\t\"name\": \"favorite_number\", \n\t\t\t \t\"type\": [\"int\", \"null\"]\n\t\t\t}, \n\t\t\t{\n\t\t\t\t\"name\": \"favorite_color\",  \n\t\t\t\t\"type\": [\"string\", \"null\"]\n\t\t\t} \n\t] \n}\n```\n\n## 2.3 编译模式\n\nAvro可以允许我们根据模式的定义而生成相应的类，一旦我们定义好相关的类，程序中就不需要直接使用模式了。可以用avro-tools jar包根据user.avsc生成User.java，语法如下:\n\n``\njava -jar avro-tools-1.7.4.jar compile schema . [注意这里有第三个参数\".\"]\n``\n\n命令执行后会在当前目录根据设定的包结构生成一个User.java类，然后就可以将定义的User对象用avro将它序列化存放到本地文件中，再将其反序列化.\n\n \n","tags":["大数据"]},{"title":"数据序列化框架对比分析","url":"/2018/06/11/数据序列化框架对比分析/","content":"# 1 概览\n由于独立于应用系统，分布式缓存的本质就是将所有的业务数据对象序列化为字节数组，然后保存到自己的内存中。所使用的序列化方案也自然会成为影响系统性能的关键点之一。一般来说，我们对一个序列化框架的关注主要有以下几点：\n1. 序列化速度：即对一个普通对象，将其从内存对象转换为字节数组需要多长时间；这个当然是越快越好  \n2. 对象压缩比：即序列化后生成对象的与原内存对象的体积比  \n3. 支持的数据类型范围：序列化框架都支持什么样的数据结构；对于大部分的序列化框架来说，都会支持普通的对象类型，但是对于复杂对象（比如说多继承关系、交叉引用、集合类等）可能不支持或支持的不够好  \n4. 易用性：一个好的序列化框架必须也是使用方便的，不需要用户做太多的依赖或者额外配置  \n\n对于一个序列化框架来说，以上几个特性很难都做到很出色，这是一个鱼和熊掌不可兼得的东西（具体原因后面会介绍），但是终归有自己的优势和特长，需要使用者根据实际场景仔细考量。下面就针对目前主流的序列化框架：Java、Hessian、ProtoBuf、Kryo分别从速度、压缩比、支持对象类型和易用性几方面分别讨论。  \n\n# 2 压缩比对比  \n单位：字节  \n\n| __工具__ | Java | Hessian | ProtoBuf | Kryo |  \n|:---------:|:----:|:-------:|:--------:|:----:|  \n|__仅数字__ |392 | 252|59 |56 |  \n| __数字 + 字符串__|494 |351 | 161| 149|  \n\n# 3 序列化速度对比  \n单位：纳秒  \n\n|__工具__|Java|Hessian|ProtoBuf|Kryo|  \n|:---------:|:----:|:-------:|:--------:|:----:|  \n|__仅数字__ |8733|6140|1154|2010|  \n| __数字 + 字符串__|12497|7863|2978|2863|  \n\n# 4 支持对象类型与易用性对比  \n## 4.1 Java源生序列化  \nava源生序列化是JDK自带的对象序列化方式，也是我们最常用的一种；其优点是简单、方便，不需要额外的依赖而且大部分三方系统或框架都支持；目前看来，Java源生序列化的兼容性也是最好的，可支持任何实现了Serializable接口的对象（包括多继承、循环引用、集合类等等）。但随之而来不可避免的就是，其序列化的速度和生成的对象体积和其他序列化框架相比，几乎都是最差的  \n## 4.2 Hessian  \nHessian的序列化实现和Java的原生序列化很相似，只是对于序列化反序列化本身并不需要的一些元数据进行了删减；所以Hessian可以像Java的源生序列化那样，可以支持任意类型的对象；但是在存储上，Hessian并没有做相应的优化，所以其生成的对象体积相较于Java的源生序列化并没有下降太多。由于Hessian相较于Java源生序列化并没有太大的优势，所以一般情况下，如果系统中没有使用Hessian的rpc框架，则很少单独使用Hessian的序列化机制。  \n## 4.3 Google Protobuf  \nGPB最大的特点就是自己定义了一套自己数据类型，并且规定只允许用我的这套；所以在使用GPB的时候，我们不得不为它单独定义一个描述文件，或者叫schema文件，用来完成Java对象中的基本数据类型和GPB自己定义的类型之间的一个映射。不过也正是GPB对类型的自定义，也让他可以更好的针对这些类型做出存储和解析上的优化，从而避免了Java源生序列化中的诸多弱点。对于对象属性，GPB并没有直接存储属性名称，而是根据schema文件中的映射关系，只保存该属性的顺序id；而对于，GPB针对常用的几种数据类型采用了不同程度的压缩，同时属性区段之间采用特定标记进行分隔，这样可以大大减少存储所占用的空间。对于数值类型，常见的压缩方式有变长byte、分组byte、差值存储等，一般都是根据属性的使用特点来做定制化的压缩策略。GPB的另一个优点就是跨语言，支持Java、C、PHP、Python等目前比较大众的语言；其他类似的还有Facebook的Thrift，也需要描述文件的支持，同时也包含了一个rpc框架和更丰富的语言支持  \n## 4.4 Kryo  \n前面我们提到，诸如Hessian和GPB这些三方的序列化框架或多或少的都对Java原生序列化机制做出了一些改进；而对于Kryo来说，改进无疑是更彻底一些；在很多评测中，Kryo的数据都是遥遥领先的；Kryo的处理和Google Protobuf类似。但有一点需要说明的是，Kryo在做序列化时，也没有记录属性的名称，而是给每个属性分配了一个id，但是他却并没有GPB那样通过一个schema文件去做id和属性的一个映射描述，所以一旦我们修改了对象的属性信息，比如说新增了一个字段，那么Kryo进行反序列化时就可能发生属性值错乱甚至是反序列化失败的情况；而且由于Kryo没有序列化属性名称的描述信息，所以序列化/反序列化之前，需要先将要处理的类在Kryo中进行注册，这一操作在首次序列化时也会消耗一定的性能。另外需要提一下的就是目前kryo目前还只支持Java语言。  \n# 参考  \n1. https://www.cnblogs.com/siqi/p/5096317.html","tags":["对比"]},{"title":"kubernetes自动扩容功能剖析","url":"/2018/06/11/kubernetes自动扩容功能剖析/","content":"# 1 概览\nkubernetes自动扩容可以根据设定的cpu利用率，来自动增加pod副本，保证每个pod的利用率均不超过设定的cpu利用率。默认的，kubernetes每隔30秒获取一次pod的cpu利用率，也就是说，当负载增加达到或者超过设定的cpu利用率，kubernetes不一定会马上增加副本，它会在它下次更新pod利用率时进行。当负载下降，同样的kubernetes又会减少增加的副本数量，但是，它也不会立即下降。一般地，这需要10-12分钟，因为kubernetes需要保证，负载下降不是这一瞬时的下降。进行扩容的部署，需要指定pod的资源请求，如果没有设定是无法进行扩容的。扩容时，获取到的cpu利用率是指所有pod的平均cpu利用率。\n\n# 2 概念解析\n## 2.1 CPU利用率\nCPU资源以cpus为单位。允许小数值。你可以用后缀m来表示mili。例如100m cpu等同于100 milicpu，意思是0.1cpu\n\n## 2.2 资源请求\n指定pod需要的资源使用，kubernetes会根据pod需求的资源量，调度到具有足够的资源请求量的节点删\n\n## 2.3 资源限制\n指定pod最大资源请求量，如果容器超出内存限制，它将结束；如果容器超出cpu限制，它将成为cpu节流的候选者\n","tags":["自动扩容"]},{"title":"高性能分布式数据缓存系统研究","url":"/2018/06/11/高性能分布式数据缓存系统研究/","content":"# 一、前言\n\n随着公司的发展，用户的规模必然越来越大，数据量也会呈指数级增加，特别是用户车辆定位数据，更是一个考验到公司后续产品发展的难题。目前，用户定位数据已经达到每天上千万条的数据量。因此，从庞大的数据池中提取数据并反馈给客户端必将成为一项挑战。同时，用户的并发访问也会考验到数据库，因为我们知道，硬盘的存取效率远远低于内存，数据库除了做了缓存以外的部分数据，大部分还是从硬盘获取，IO瓶颈制约着对用户的实时并发响应。为了解决上述问题，设计一套高并发、高吞吐量的数据缓存系统成为一件迫切的事。基于此，我们需要研究分析分布式数据缓存系统的现状，在此基础上，再设计一套符合公司定位的分布式数据缓存系统。\n\n# 二、数据库的瓶颈\n## 2.1数据量\n\n关系型数据库的数据量是比较小的，以我们常用的MySQL为例，单表数据条数一般应该控制在2000w以内，如果业务很复杂的话，可能还要低一些。即便是对于Oracle这些大型商业数据库来讲，其能存储的数据量也很难满足一个拥有几千万甚至数亿用户的大型互联网系统。\n\n## 2.2TPS\n在实际开发中我们经常会发现，关系型数据库在TPS上的瓶颈往往会比其他瓶颈更容易暴露出来，尤其对于大型web系统，由于每天大量的并发访问，对数据库的读写性能要求非常高；而传统的关系型数据库的处理能力确实捉襟见肘；以我们常用的MySQL数据库为例，常规情况下的TPS大概只有1500左右（各种极端场景下另当别论）；下图是MySQL官方所给出的一份测试数据：\n\n\n![](/images/318497-20160103141339432-1681082715.jpg)\n\n而对于一个日均PV千万的大型网站来讲，每个PV所产生的数据库读写量可能要超出几倍，这种情况下，每天所有的数据读写请求量可能远超出关系型数据的处理能力，更别说在流量峰值的情况下了；所以我们必须要有高效的缓存手段来抵挡住大部分的数据请求！\n\n## 2.3响应时间\n正常情况下，关系型数据的响应时间是相当不错的，一般在10ms以内甚至更短，尤其是在配置得当的情况下。但是就如前面所言，我们的需求是不一般的：当拥有几亿条数据，1wTPS的时候，响应时间也要在10ms以内，这几乎是任何一款关系型数据都无法做到的。\n那么这个问题如何解决呢？最简单有效的办法当然是缓存！\n\n# 三、缓存系统选型\n\n\n## 3.1缓存的类型\n\n### 3.1.1本地缓存\n\n本地缓存可能是大家用的最多的一种缓存方式了，不管是本地内存还是磁盘，其速度快，成本低，在有些场合非常有效；但是对于web系统的集群负载均衡结构来说，本地缓存使用起来就比较受限制，因为当数据库数据发生变化时，你没有一个简单有效的方法去更新本地缓存；然而，你如果在不同的服务器之间去同步本地缓存信息，由于缓存的低时效性和高访问量的影响，其成本和性能恐怕都是难以接受的。\n\n### 3.1.2分布式缓存\n\n本地缓存的使用很容易让你的应用服务器带上“状态”，这种情况下，数据同步的开销会比较大；尤其是在集群环境中更是如此！\n\n分布式缓存这种东西存在的目的就是为了提供比RDB更高的TPS和扩展性，同时有帮你承担了数据同步的痛苦；优秀的分布式缓存系统有大家所熟知的Memcached、Redis（当然也许你把它看成是NoSQL，但是我个人更愿意把分布式缓存也看成是NoSQL），还有国内阿里自主开发的Tair等。\n\n对比关系型数据库和缓存存储，其在读和写性能上的差距可谓天壤之别；memcached单节点已经可以做到15w以上的tps、Redis、google的levelDB也有不菲的性能，而实现大规模集群后，性能可能会更高！所以，在技术和业务都可以接受的情况下，我们可以尽量把读写压力从数据库转移到缓存上，以保护看似强大，其实却很脆弱的关系型数据库。\n\n### 3.1.3数据库缓存\n主要指数据库的查询缓存，大部分数据库都是会提供，每种数据库的具体实现细节也会有所差异，不过基本的原理就是用查询语句的hash值做key，对结果集进行缓存；如果利用的好，可以很大的提高数据库的查询效率！\n\n## 3.2选型指标\n### 3.2.1容量\n\n容量当然是越大越好了。每个系统在初期规划的时候，都会大致计算一下所要消耗的缓存空间，这主要取决于你要缓存的对象数量和单个对象的大小。一般来说，你可以采用对象属性在内存中的存储长度简单加和的方法来计算单个对象的体积，再乘以缓存对象的数量和预期增长（当然，这里边有一个热点数据的问题，这里就不细讨论了），大概得出需要使用的缓存空间；之后就可以按照这个指标去申请缓存空间或搭建缓存系统了。\n\n### 3.2.2并发量\n\n其实这里的并发量改为QPS更为贴切，因为我们的缓存不是直接面向用户的，而是面向应用的。所以我们关心的是一个缓存系统平均每秒能够承受多少的访问量。\n\n我们之所以需要缓存系统，就是要它在关键时刻能抗住我们的数据访问量的；所以，缓存系统能够支撑的并发量是一个非常重要的指标，如果它的性能还不如关系型数据库，那我们就没有使用的必要了。\n\n对于淘宝的系统来说，我们不妨按照下边的方案来估算并发量：\nQPS = 日PV × 读写次数/PV ÷ (8 × 60 × 60)\n这里我们是按照一天8个小时来计算的，这个值基于一个互联网站点的访问规律得出的。\n在估算访问量的时候，我们不得不考虑一个峰值的问题，尤其是像淘宝、京东这样大型的电商网站，经常会因为一些大的促销活动而使PV、UV冲到平时的几倍甚至几十倍，这也正是缓存系统发挥作用的关键时刻；倍受瞩目的12306在站点优化过程中也大量的引入了缓存（内存文件系统）来提升性能。\n在计算出平均值之后，再乘以一个峰值系数，基本就可以得出你的缓存系统需要承受的最高QPS，一般情况下，这个系数定在10以内是合理的。\n\n### 3.2.3响应时间\n响应时间当然也是必要的，如果一个缓存系统慢的跟蜗牛一样，甚至直接就超时了，那和我们使用MySQL也没啥区别了。\n\n一般来说，要求一个缓存系统在1ms或2ms之内返回数据是不过分的，当然前提是你的数据不会太大；如果想更快的话，那你就有点过分了，除非你是用的本地缓存；因为一般而言，在大型IDC内部，一个TCP回环（不携带业务数据）差不多就要消耗掉0.2ms至0.5ms。\n\n大部分的缓存系统，由于是基于内存，所以响应时间都很短，但是问题一般会出现在数据量和QPS变大之后，由于内存管理策略、数据查找方式、I/O模型、业务场景等方面的差异，响应时间可能会差异很多，所以对于QPS和响应时间这两项指标，还要靠上线前充分的性能测试来进一步确认，不能只单纯的依赖官方的测试结果。  \n\n### 3.2.4使用成本\n\n优秀的系统要是能够方便部署和方便运维的，不需要高端硬件、不需要复杂的环境配置、不能有过多的依赖条件，同时还要稳定、易维护\n\n### 3.2.5扩展空间\n\n缓存系统的扩展性是指在空间不足的性情况，能够通过增加机器等方式快速的在线扩容。这也是能够支撑业务系统快速发展的一个重要因素\n\n### 3.2.6容灾\n\n我们使用缓存系统的初衷就是当数据请求量很大，数据库无法承受的情况，能够通过缓存来抵挡住大部分的请求流量，所以一旦缓存服务器发生故障，而缓存系统又没有一个很好的容灾措施的话，所有或部分的请求将会直接压倒数据库上，这可能会直接导致DB崩溃。\n并不是所有的缓存系统都具有容灾特性的，所以我们在选择的时候，一定要根据自己的业务需求，对缓存数据的依赖程度来决定是否需要缓存系统的容灾特性。\n\n## 3.3常见分布式缓存系统对比\n### 3.3.1Memcached\n\nMemcached严格的说还不能算是一个分布式缓存系统，但由于Memcached的开源，其访问协议也都是公开的，所以目前有很多第三方的客户端或扩展，在一定程度上对Memcached的集群扩展做了支持，但是大部分都只是做了一个简单Hash或者一致性Hash。\n由于Memcached内部通过固定大小的chunk链的方式去管理内存数据，分配和回收效率很高，所以其读写性能也非常高。官方给出的数据，64KB对象的情况下，单机QPS可达到15w以上。\n\nMemcached集群的不同机器之间是相互独立的，没有数据方面的通信，所以也不具备failover的能力，在发生数据倾斜的时候也无法自动调整。\n\nMemcached的多语言支持非常好，目前可支持C/C++、Java、C#、PHP、Python、Perl、Ruby等常用语言，也有大量的文档和示例代码可供参考，而且其稳定性也经过了长期的检验，应该说比较适合于中小型系统和初学者使用的缓存系统。\n\n### 3.3.2Redis\nRedis被看作是Memcached的替代品。\n\nRedis除了像Memcached那样支持普通的<k,v>类型的存储外，还支持List、Set、Map等集合类型的存储，这种特性有时候在业务开发中会比较方便。\n\nRedis源生支持持久化存储，但是根据很多人的使用情况和测试结果来看，Redis的持久化性能不是很理想，官方也不推荐过度依赖Redis持久化存储功能。就性能来讲，在全部命中缓存时，Redis的性能接近memcached，但是一旦使用了持久化之后，性能会迅速下降，甚至会相差一个数量级。\n\n### 3.3.3淘宝Tair\n\nTair是淘宝自主开发并开源的一款的缓存系统，是一套真正意义上的分布式并且可以跨多机房部署，同时支持内存缓存和持久化存储的解决方案。\n\nTair实现了缓存框架和缓存存储引擎的独立，在遵守接口规范的情况下，可以根据需求更换存储引擎，目前支持mdb（基于memcached）、rdb（基于Redis）、kdb（基于kyoto cabinet，持久存储，目前已不推荐使用）和rdb（基于gooogle的levelDB，持久化存储）几种引擎\n\n由于基于mdb和rdb，所以Tair能够间距两者的特性，在并发量和响应时间上，接近二者的裸系统\n\n在扩展性和容灾方面，Tair自己做了增强；通过使用虚拟节点Hash（一致性Hash的变种实现）的方案，将key通过Hash函数映射到到某个虚拟节点（桶）上，然后通过中心服务器（configserver）来管理虚拟节点到物理节点的映射关系。这样，Tair不但实现了基于Hash的首次负载均衡，同时又可以通过调整虚拟节点和物理节点的映射关系来实现二次负载均衡，这样有效的解决了由于业务热点导致的访问不均衡问题以及线性扩容时数据迁移麻烦；此外，Tair的每台缓存服务器和中心服务器（configserver）也有主备设计，所以其可用性也大大提高。\n\n\n## 3.4缓存的设计与策略\n### 3.4.1缓存对象设计\n#### 3.4.1.1缓存对象粒度\n\n对于本地磁盘或分布是缓存系统来说，其缓存的数据一般都不是结构化的，而是半结构话或是序列化的；这就导致了我们读取缓存时，很难直接拿到程序最终想要的结果；这就像快递的包裹，如果你不打开外层的包装，你就拿不出来里边的东西.如果包裹里的东西有很多，但是其中只有一个是你需要的，其他的还要再包好送给别人；这时候你打开包裹时就会很痛苦——为了拿到自己的东西，必须要拆开包裹，但是拆开后还要很麻烦的将剩下的再包会去；等包裹传递到下一个人的手里，又是如此.\n\n所以，这个时候粒度的控制就很重要了；到底是一件东西就一个包裹呢，还是好多东西都包一块呢？前者拆起来方便，后着节约包裹数量。映射到我们的系统上，我们的缓存对象中到底要放哪些数据？一种数据一个对象，简单，读取写入都快，但是种类一多，缓存的管理成本就会很高；多种数据放在一个对象里，方便，一块全出来了，想用哪个都可以，但是如果我只要一种数据，其他的就都浪费了，网络带宽和传输延迟的消耗也很可观。\n这个时候主要的考虑点就应该是业务场景了，不同的场景使用不同的缓存粒度，折衷权衡；不要不在乎这点性能损失，缓存一般都是访问频率非常高的数据，各个点的累积效应可能是非常巨大的！\n\n当然，有些缓存系统的设计也要求我们必须考虑缓存对象的粒度问题；比如说Memcached，其chunk设计要求业务要能很好的控制其缓存对象的大小；淘宝的Tair也是，对于尺寸超过1M的对象，处理效率将大为降低.\n像Redis这种提供同时提供了Map、List结构支持的系统来说，虽然增加了缓存结构的灵活性，但最多也只能算是半结构化缓存，还无法做到像本地内存那样的灵活性。\n\n粒度设计的过粗还会遇到并发问题。一个大对象里包含的多种数据，很多地方多要用，这时如果使用的是缓存修改模式而不是过期模式，那么很可能会因为并发更新而导致数据被覆盖；版本控制是一种解决方法，但是这样会使缓存更新失败的概率大大增加，而且有些缓存系统也不提供版本支持（比如说用的很广泛的Memcached）。\n\n#### 3.4.1.2 缓存对象结构\n同缓存粒度一样，缓存的结构也是一样的道理。对于一个缓存对象来说，并不是其粒度越小，体积也越小；如果你的一个字符串就有1M大小，那也是很恐怖的.\n\n数据的结构决定着你读取的方式，举个很简单的例子，集合对象中，List和Map两种数据结构，由于其底层存储方式不同，所以使用的场景也不一样；前者更适合有序遍历，而后者适合随机存取；回想一下，你是不是曾经在程序中遇到过为了merge两个list中的数据，而不得不循环嵌套？\n\n所以，根据具体应用场景去为缓存对象设计一个更合适的存储结构，也是一个很值得注意的点。\n\n### 3.4.2 缓存更新策略\n\n缓存的更新策略主要有两种：被动失效和主动更新，下面分别进行介绍.\n\n#### 3.4.2.1 被动失效\n\n一般来说，缓存数据主要是服务读请求的，并设置一个过期时间；或者当数据库状态改变时，通过一个简单的delete操作，使数据失效掉；当下次再去读取时，如果发现数据过期了或者不存在了，那么就重新去持久层读取，然后更新到缓存中；这即是所谓的被动失效策略。\n\n但是在被动失效策略中存在一个问题，就是从缓存失效或者丢失开始直到新的数据再次被更新到缓存中的这段时间，所有的读请求都将会直接落到数据库上；而对于一个大访问量的系统来说，这有可能会带来风险。所以我们换一种策略就是，当数据库更新时，主动去同步更新缓存，这样在缓存数据的整个生命期内，就不会有空窗期，前端请求也就没有机会去亲密接触数据库。\n\n#### 3.4.2.2 主动更新\n\n前面我们提到主动更新主要是为了解决空窗期的问题，但是这同样会带来另一个问题，就是并发更新的情况.\n在集群环境下，多台应用服务器同时访问一份数据是很正常的，这样就会存在一台服务器读取并修改了缓存数据，但是还没来得及写入的情况下，另一台服务器也读取并修改旧的数据，这时候，后写入的将会覆盖前面的，从而导致数据丢失；这也是分布式系统开发中，必然会遇到的一个问题。解决的方式主要有三种：\n\n 1.__锁控制__：这种方式一般在客户端实现（在服务端加锁是另外一种情况），其基本原理就是使用读写锁，即任何进程要调用写方法时，先要获取一个排他锁，阻塞住所有的其他访问，等自己完全修改完后才能释放；如果遇到其他进程也正在修改或读取数据，那么则需要等待。锁控制虽然是一种方案，但是很少有真的这样去做的，其缺点显而易见，其并发性只存在于读操作之间，只要有写操作存在，就只能串行。\n \n 2.__版本控制__：这种方式也有两种实现，一种是单版本机制，即为每份数据保存一个版本号，当缓存数据写入时，需要传入这个版本号，然后服务端将传入的版本号和数据当前的版本号进行比对，如果大于当前版本，则成功写入，否则返回失败；这样解决方式比较简单；但是增加了高并发下客户端的写失败概率.还有一种方式就是多版本机制，即存储系统为每个数据保存多份，每份都有自己的版本号，互不冲突，然后通过一定的策略来定期合并，再或者就是交由客户端自己去选择读取哪个版本的数据。很多分布式缓存一般会使用单版本机制，而很多NoSQL则使用后者.\n\n### 3.4.3 数据对象序列化\n由于独立于应用系统，分布式缓存的本质就是将所有的业务数据对象序列化为字节数组，然后保存到自己的内存中。所使用的序列化方案也自然会成为影响系统性能的关键点之一。    \n\n一般来说，我们对一个序列化框架的关注主要有以下几点：\n\n 1.__序列化速度__：即对一个普通对象，将其从内存对象转换为字节数组需要多长时间；这个当然是越快越好\n     \n 2.__对象压缩比__：即序列化后生成对象的与原内存对象的体积比  \n \n 3.__支持的数据类型范围__：序列化框架都支持什么样的数据结构；对于大部分的序列化框架来说，都会支持普通的对象类型，但是对于复杂对象（比如说多继承关系、交叉引用、集合类等）可能不支持或支持的不够好\n     \n 4.__易用性__：一个好的序列化框架必须也是使用方便的，不需要用户做太多的依赖或者额外配置\n     \n对于一个序列化框架来说，以上几个特性很难都做到很出色，这是一个鱼和熊掌不可兼得的东西（具体原因后面会介绍），但是终归有自己的优势和特长，需要使用者根据实际场景仔细考量。序列化工具对比请参考另一篇《序列化工具对比》.\n\n# 参考\n\n\n 1. https://www.cnblogs.com/siqi/p/5096317.html\n 2. https://segmentfault.com/a/1190000003985468\n 3. https://blog.csdn.net/damacheng/article/details/42846549\n 4. https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines\n\n \n\n \n","tags":["缓存"]},{"title":"Kafka高性能吞吐揭秘","url":"/2018/06/11/Kafka高性能吞吐揭秘/","content":"# 概览\nkafka是一套分布式高性能消息缓冲集群系统，其中涉及到的名词如下所示：  \n - Topic：用于划分Message的逻辑概念，一个Topic可以分布在多个Broker上\n - Partition：是Kafka中横向扩展和一切并行化的基础，每个Topic都至少被切分为1个Partition\n - Offset：消息在Partition中的编号，编号顺序不跨Partition\n - Consumer：用于从Broker中取出/消费Message\n - Producer：用于往Broker中发送/生产Message\n - Replication：Kafka支持以Partition为单位对Message进行冗余备份，每个Partition都可以配置至少1个Replication(当仅1个Replication时即仅该Partition本身)\n - Leader：每个Replication集合中的Partition都会选出一个唯一的Leader，所有的读写请求都由Leader处理。其他Replicas从Leader处把数据更新同步到本地，过程类似大家熟悉的MySQL中的Binlog同步\n - Broker：Kafka中使用Broker来接受Producer和Consumer的请求，并把Message持久化到本地磁盘。每个Cluster当中会选举出一个Broker来担任Controller，负责处理Partition的Leader选举，协调Partition迁移等工作\n - ISR(In-Sync Replica)：是Replicas的一个子集，表示目前Alive且与Leader能够“Catch-up”的Replicas集合。由于读写都是首先落到Leader上，所以一般来说通过同步机制从Leader上拉取数据的Replica都会和Leader有一些延迟(包括了延迟时间和延迟条数两个维度)，任意一个超过阈值都会把该Replica踢出ISR。每个Partition都有它自己独立的ISR\n# Broker\n\n不同于Redis和MemcacheQ等内存消息队列，Kafka的设计是把所有的Message都要写入速度低容量大的硬盘，以此来换取更强的存储能力。实际上，Kafka使用硬盘并没有带来过多的性能损失，“规规矩矩”的抄了一条“近道”。\n  \n首先，说“规规矩矩”是因为Kafka在磁盘上只做Sequence I/O，由于消息系统读写的特殊性，这并不存在什么问题。关于磁盘I/O的性能，引用一组Kafka官方给出的测试数据(Raid-5，7200rpm):\n- Sequence I/O: 600MB/s  \n- Random I/O: 100KB/s \n\n所以通过只做Sequence I/O的限制，规避了磁盘访问速度低下对性能可能造成的影响。  \n\n那么kafka是如何抄近道的呢？首先，Kafka重度依赖底层操作系统提供的PageCache功能。当上层有写操作时，操作系统只是将数据写入PageCache，同时标记Page属性为Dirty。当读操作发生时，先从PageCache中查找，如果发生缺页才进行磁盘调度，最终返回需要的数据。实际上PageCache是把尽可能多的空闲内存都当做了磁盘缓存来使用。同时如果有其他进程申请内存，回收PageCache的代价又很小，所以现代的OS都支持PageCache。使用PageCache功能同时可以避免在JVM内部缓存数据，JVM为我们提供了强大的GC能力，同时也引入了一些问题不适用与Kafka的设计。如果在Heap内管理缓存，JVM的GC线程会频繁扫描Heap空间，带来不必要的开销。如果Heap过大，执行一次Full GC对系统的可用性来说将是极大的挑战。所有在在JVM内的对象都不免带有一个Object Overhead(千万不可小视)，内存的有效空间利用率会因此降低。所有的In-Process Cache在OS中都有一份同样的PageCache。所以通过将缓存只放在PageCache，可以至少让可用缓存空间翻倍。如果Kafka重启，所有的In-Process Cache都会失效，而OS管理的PageCache依然可以继续使用。PageCache还只是第一步，Kafka为了进一步的优化性能还采用了Sendfile技术。在解释Sendfile之前，首先介绍一下传统的网络I/O操作流程，大体上分为以下4步：\n\n 1. OS 从硬盘把数据读到内核区的PageCache\n 2. 用户进程把数据从内核区Copy到用户区\n 3. 然后用户进程再把数据写入到Socket，数据流入内核区的Socket Buffer上\n 4. OS 再把数据从Buffer中Copy到网卡的Buffer上，这样完成一次发送\n\n\n![](/images/1021389888-56442ffade963.jpg)\n\n整个过程共经历两次Context Switch，四次System Call。同一份数据在内核Buffer与用户Buffer之间重复拷贝，效率低下。其中2、3两步没有必要，完全可以直接在内核区完成数据拷贝。这也正是Sendfile所解决的问题，经过Sendfile优化后，整个I/O过程就变成了下面这个样子。 \n\n![](/images/4284736387-56443027674a2.jpg)\n\n通过以上的介绍不难看出，Kafka的设计初衷是尽一切努力在内存中完成数据交换，无论是对外作为一整个消息系统，或是内部同底层操作系统的交互。如果Producer和Consumer之间生产和消费进度上配合得当，完全可以实现数据交换零I/O。这也就是我为什么说Kafka使用“硬盘”并没有带来过多性能损失的原因。下面是在生产环境中采到的一些指标。\n(20 Brokers, 75 Partitions per Broker, 110k msg/s) \n\n![](/images/1593922197-5644305578f81.jpg)\n\n此时的集群只有写，没有读操作。10M/s左右的Send的流量是Partition之间进行Replicate而产生的。从recv和writ的速率比较可以看出，写盘是使用Asynchronous+Batch的方式，底层OS可能还会进行磁盘写顺序优化。而在有Read Request进来的时候分为两种情况，第一种是内存中完成数据交换。\n\n \n![](/images/1537103758-564430720edc9.jpg)\n\nSend流量从平均10M/s增加到了到平均60M/s，而磁盘Read只有不超过50KB/s。PageCache降低磁盘I/O效果非常明显。接下来是读一些收到了一段时间，已经从内存中被换出刷写到磁盘上的老数据。 \n\n![](/images/1836079238-56443081660f3.jpg)\n\n其他指标还是老样子，而磁盘Read已经飚高到40+MB/s。此时全部的数据都已经是走硬盘了(对硬盘的顺序读取OS层会进行Prefill PageCache的优化)。依然没有任何性能问题。\n   \n__Tips__：\n\n 1. Kafka官方并不建议通过Broker端的log.flush.interval.messages和log.flush.interval.ms来强制写盘，认为数据的可靠性应该通过Replica来保证，而强制Flush数据到磁盘会对整体性能产生影响\n 2. 可以通过调整/proc/sys/vm/dirty_background_ratio和/proc/sys/vm/dirty_ratio来调优性能\n 3. 脏页率超过第一个指标会启动pdflush开始Flush Dirty PageCache\n 4. 脏页率超过第二个指标会阻塞所有的写操作来进行Flush\n 5. 根据不同的业务需求可以适当的降低dirty_background_ratio和提高dirty_ratio\n\n# Partition\nPartition是Kafka可以很好的横向扩展和提供高并发处理以及实现Replication的基础。\n  \n扩展性方面。首先，Kafka允许Partition在集群内的Broker之间任意移动，以此来均衡可能存在的数据倾斜问题。其次，Partition支持自定义的分区算法，例如可以将同一个Key的所有消息都路由到同一个Partition上去。 同时Leader也可以在In-Sync的Replica中迁移。由于针对某一个Partition的所有读写请求都是只由Leader来处理，所以Kafka会尽量把Leader均匀的分散到集群的各个节点上，以免造成网络流量过于集中。\n  \n并发方面。任意Partition在某一个时刻只能被一个Consumer Group内的一个Consumer消费(反过来一个Consumer则可以同时消费多个Partition)，Kafka非常简洁的Offset机制最小化了Broker和Consumer之间的交互，这使Kafka并不会像同类其他消息队列一样，随着下游Consumer数目的增加而成比例的降低性能。此外，如果多个Consumer恰巧都是消费时间序上很相近的数据，可以达到很高的PageCache命中率，因而Kafka可以非常高效的支持高并发读操作，实践中基本可以达到单机网卡上限。\n   \n不过，Partition的数量并不是越多越好，Partition的数量越多，平均到每一个Broker上的数量也就越多。考虑到Broker宕机(Network Failure, Full GC)的情况下，需要由Controller来为所有宕机的Broker上的所有Partition重新选举Leader，假设每个Partition的选举消耗10ms，如果Broker上有500个Partition，那么在进行选举的5s的时间里，对上述Partition的读写操作都会触发LeaderNotAvailableException。  \n再进一步，如果挂掉的Broker是整个集群的Controller，那么首先要进行的是重新任命一个Broker作为Controller。新任命的Controller要从Zookeeper上获取所有Partition的Meta信息，获取每个信息大概3-5ms，那么如果有10000个Partition这个时间就会达到30s-50s。而且不要忘记这只是重新启动一个Controller花费的时间，在这基础上还要再加上前面说的选举Leader的时间 -_-!!!!!!  \n此外，在Broker端，对Producer和Consumer都使用了Buffer机制。其中Buffer的大小是统一配置的，数量则与Partition个数相同。如果Partition个数过多，会导致Producer和Consumer的Buffer内存占用过大。  \n__Tips__：\n\n 1. Partition的数量尽量提前预分配，虽然可以在后期动态增加Partition，但是会冒着可能破坏Message Key和Partition之间对应关系的风险\n 2. Replica的数量不要过多，如果条件允许尽量把Replica集合内的Partition分别调整到不同的Rack\n 3. 尽一切努力保证每次停Broker时都可以Clean Shutdown，否则问题就不仅仅是恢复服务所需时间长，还可能出现数据损坏或其他很诡异的问题\n\n# Producer\n\nKafka的研发团队表示在0.8版本里用Java重写了整个Producer，据说性能有了很大提升。我还没有亲自对比试用过，这里就不做数据对比了。本文结尾的扩展阅读里提到了一套我认为比较好的对照组，有兴趣的同学可以尝试一下。  \n其实在Producer端的优化大部分消息系统采取的方式都比较单一，无非也就化零为整、同步变异步这么几种。  \nKafka系统默认支持MessageSet，把多条Message自动地打成一个Group后发送出去，均摊后拉低了每次通信的RTT。而且在组织MessageSet的同时，还可以把数据重新排序，从爆发流式的随机写入优化成较为平稳的线性写入。  \n此外，还要着重介绍的一点是，Producer支持End-to-End的压缩。数据在本地压缩后放到网络上传输，在Broker一般不解压(除非指定要Deep-Iteration)，直至消息被Consume之后在客户端解压。  \n当然用户也可以选择自己在应用层上做压缩和解压的工作(毕竟Kafka目前支持的压缩算法有限，只有GZIP和Snappy)，不过这样做反而会意外的降低效率！！！！ Kafka的End-to-End压缩与MessageSet配合在一起工作效果最佳，上面的做法直接割裂了两者间联系。至于道理其实很简单，压缩算法中一条基本的原理“重复的数据量越多，压缩比越高”。无关于消息体的内容，无关于消息体的数量，大多数情况下输入数据量大一些会取得更好的压缩比。   \n不过Kafka采用MessageSet也导致在可用性上一定程度的妥协。每次发送数据时，Producer都是send()之后就认为已经发送出去了，但其实大多数情况下消息还在内存的MessageSet当中，尚未发送到网络，这时候如果Producer挂掉，那就会出现丢数据的情况。  \n为了解决这个问题，Kafka在0.8版本的设计借鉴了网络当中的ack机制。如果对性能要求较高，又能在一定程度上允许Message的丢失，那就可以设置request.required.acks=0 来关闭ack，以全速发送。如果需要对发送的消息进行确认，就需要设置request.required.acks为1或-1，那么1和-1又有什么区别呢？这里又要提到前面聊的有关Replica数量问题。如果配置为1，表示消息只需要被Leader接收并确认即可，其他的Replica可以进行异步拉取无需立即进行确认，在保证可靠性的同时又不会把效率拉得很低。如果设置为-1，表示消息要Commit到该Partition的ISR集合中的所有Replica后，才可以返回ack，消息的发送会更安全，而整个过程的延迟会随着Replica的数量正比增长，这里就需要根据不同的需求做相应的优化。   \n__Tips__：\n1. Producer的线程不要配置过多，尤其是在Mirror或者Migration中使用的时候，会加剧目标集群Partition消息乱序的情况(如果你的应用场景对消息顺序很敏感的话)\n2. 0.8版本的request.required.acks默认是0(同0.7)\n\n# Consumer\nConsumer端的设计大体上还算是比较常规的。\n\n- 通过Consumer Group，可以支持生产者消费者和队列访问两种模式\n- Consumer API分为High level和Low level两种。前一种重度依赖Zookeeper，所以性能差一些且不自由，但是超省心。第二种不依赖Zookeeper服务，无论从自由度和性能上都有更好的表现，但是所有的异常(Leader迁移、Offset越界、Broker宕机等)和Offset的维护都需要自行处理\n- 大家可以关注下不日发布的0.9 Release。开发人员又用Java重写了一套Consumer。把两套API合并在一起，同时去掉了对Zookeeper的依赖。据说性能有大幅度提升\n\n__Tips__：\n\n强烈推荐使用Low level API，虽然繁琐一些，但是目前只有这个API可以对Error数据进行自定义处理，尤其是处理Broker异常或由于Unclean Shutdown导致的Corrupted Data时，否则无法Skip只能等着“坏消息”在Broker上被Rotate掉，在此期间该Replica将会一直处于不可用状态。\n\n# 参考\n 1. https://segmentfault.com/a/1190000003985468\n \n \n","tags":["消息队列"]},{"title":"Hello World","url":"/2018/06/11/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","tags":["说明"]}]